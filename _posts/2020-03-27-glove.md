# GLObal VEctors for word representations

1. TOC
{:toc}

## Questions:
How meaning is generated from word occurence statistics?

Local and global corpus statistics?

## Notation

$X$ : Co-occurrence matrix of words

$X_{ij}$ : number of times word j occurs in the context of word i.

$X_{i} = \sum_{k}X_{ik}$ be the number of times any word appears in the context of word $i$.

$P_{ij}=P(i \mid j)=X_{ij}/X_{i}$ be the probability that word $j$ appears in the context of word $i$.

COP = Co-occurrence probability.

## How meaning can be extracted from cooccurrences

Take a look at the image below:

![co-occ-probs](/images/co-occ-probs.png)

For example consider two words `ice` and `steam` denoted by $i$ and $j$ resp and let the "probe" word k be any of the : `solid`,  `gas`, `water` and `fashion`.


Let's analyze co-occurence probabilities of words $i$ and $j$ with above probe words. The exact values can be found in the above image, these values are obtained from a large corpus. Let's analyze them.

  1. For words related to `ice` but not `steam`, we expect $P_{ik} > P_{jk}$, $k$ being probe words. This can be verified by looking at $k=solid$. We find the ratio $P_{ij}/P_{jk} = 8.9 > 1$.
  2. For words related to `steam` but not `ice`, we expect $P_{ik} < P_{jk}$, $k$ being probe words. This can be verified by looking at $k=gas$. We find the ratio $P_{ij}/P_{jk} = 8.5*10^{-2} < 1$.
  3. For equally related words, $P_{ik} \approx P_{jk} $ this means $P_{ik}/P_{jk} \to 1$. This can be observed for $k=water$ because water is related to both `ice` and `steam`.
  4. For equally unrelated words, $P_{ik} \approx P_{jk} \to 0$ but $P_{ik}/P_{jk} \to 1$. This can be observed for $k=fashion$ because fashion is eqaully un-correlated to either `ice` or `steam`.

### Why do we care about the ratio of COPs and not COPs themselves?

This is because ratio is better able to distinguish relevant words from irrevant words. 

1. For relevant words: 
  1. $P_{ik}/P_{jk} >1$ if $k$ is related to $i$ but not $j$.
  2. $P_{ik}/P_{jk} <1$ if $k$ is related to $j$ but not $i$.
2. For irrelevant words or equally relevant words, $P_{ik}/P_{jk} \to 1$.
